{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wQixnjTn2arY"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "def unique(list1):\n",
    "    list_set = set(list1)\n",
    "    unique_list = (list(list_set))\n",
    "    unique_list.sort()\n",
    "    return unique_list\n",
    "    \n",
    "\n",
    "def linear(arr):\n",
    "  res = []\n",
    "  for el in arr :\n",
    "    res.append(el)\n",
    "  return res\n",
    "\n",
    "def sigmoid(arr):\n",
    "  res = []\n",
    "  for el in arr:\n",
    "    res.append(1 / (1 + math.exp(-el)))\n",
    "  return res\n",
    "\n",
    "def reLu(arr):\n",
    "  res = []\n",
    "  for el in arr:\n",
    "    res.append(max(0, el))\n",
    "  return res\n",
    "  \n",
    "def softMax(x):\n",
    "  e_x = np.exp(x - np.max(x))\n",
    "  return (e_x / e_x.sum()).tolist()\n",
    "\n",
    "def arrayMultiplication(x_array, w_array, length):\n",
    "  oi = 0\n",
    "  for i in range(length):\n",
    "    oi += x_array[i] * w_array[i]\n",
    "  return oi\n",
    "\n",
    "# For Output Layer\n",
    "# d(Ed)/d(Wji) = d(Ed)/d(Oj) * d(Oj)/d(NETj) * d(NETj)/d(Wji)\n",
    "def derivate_Ed_To_Oj(target_j, output_j, activationFunction):\n",
    "  if (activationFunction != 4):\n",
    "    return (-1) * (target_j - output_j)\n",
    "  else : # Help ini\n",
    "    return \"APA INIIIIII\"\n",
    "\n",
    "def derivate_Oj_To_NETj(output_j, activationFunction):\n",
    "  if (activationFunction == 2):\n",
    "    return output_j * (1 - output_j)\n",
    "  elif (activationFunction == 1):\n",
    "    return 1\n",
    "  elif (activationFunction == 3):\n",
    "    return 1 if (output_j) >= 0 else 0\n",
    "  else : # Softmax buat target class, ryan tolong isi\n",
    "    return 0 # Isi sama turunan softmax\n",
    "\n",
    "def getErrorNodeOutput(target_j, output_j, activationFunction):\n",
    "  return (-1) * (derivate_Ed_To_Oj(target_j, output_j, activationFunction) * derivate_Oj_To_NETj(output_j, activationFunction))\n",
    "\n",
    "# For Hidden Layer\n",
    "# d(Ed)/d(Wji) = d(Ed)/d(NETj) * d(NETj)/d(Wji)\n",
    "# With d(Ed)/d(NETj) = d(Ed)/d(NETk) * d(NETk)/d(Oj) * d(Oj)/d(NETj)\n",
    "# Thus -> d(Ed)/d(Wji) = d(Ed)/d(NETk) * d(NETk)/d(Oj) * d(Oj)/d(NETj)* d(NETj)/d(Wji)\n",
    "\n",
    "# For Softmax\n",
    "def derivate_Ed_To_NETj(activationFunction, sigma,  output_j): # Ini masih gatau bray\n",
    "  return sigma * derivate_Oj_To_NETj(output_j, activationFunction)\n",
    "\n",
    "def derivate_Ed_To_NETj_Softmax(pj, label, j):\n",
    "  if (j != label):\n",
    "    return pj\n",
    "  else:\n",
    "    return -(1 - pj)\n",
    "\n",
    "# def derivate_Ed_To_NETk(errorNode_k):\n",
    "#   return (-1) * (errorNode_k)\n",
    "\n",
    "# def derivate_NETk_To_Oj(w_kj):\n",
    "#   return w_kj\n",
    "\n",
    "# BACA INI\n",
    "# Get Error Node for Hidden Neuron bikin di MBGD karena ada softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wTMN3RPs2bAF"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Neuron:\n",
    "  \n",
    "  def __init__(self, actType, bias=1):\n",
    "    self.weight = []\n",
    "    self.bias = bias\n",
    "    self.accumulative_delta = []\n",
    "    self.output = 0\n",
    "\n",
    "  def setWeight(self, weight):\n",
    "    self.weight = weight\n",
    "\n",
    "  def setErrorNode(self, errorNode):\n",
    "    self.errorNode = errorNode\n",
    "\n",
    "  def getWeight(self, index):\n",
    "    return self.weight[index]\n",
    "  \n",
    "  def getWeightArray(self):\n",
    "    return self.weight\n",
    "  \n",
    "  def getAccumulativeDelta(self):\n",
    "    return self.accumulative_delta\n",
    "  \n",
    "  def setAccumulativeDelta(self, accumulative_delta):\n",
    "    self.accumulative_delta = accumulative_delta\n",
    "  \n",
    "  def getErrorNode(self):\n",
    "    return self.errorNode\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zUy8qSMo2bHB"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Layer:\n",
    "  \n",
    "  def __init__(self, bias):\n",
    "    self.neuron_array = []\n",
    "    self.input_array = []\n",
    "    self.output_array = []\n",
    "    self.type = 1 # default Linear\n",
    "    self.bias = bias\n",
    "    self.activationFunction = linear\n",
    "\n",
    "  def setType(self, layerType):\n",
    "    self.type = layerType\n",
    "    if(layerType == 1):\n",
    "      self.activationFunction = linear\n",
    "    elif(layerType == 2):\n",
    "      self.activationFunction = sigmoid\n",
    "    elif(layerType == 3):\n",
    "      self.activationFunction = reLu\n",
    "    elif(layerType == 4):\n",
    "      self.activationFunction = softMax\n",
    "    else:\n",
    "      self.activationFunction = sigmoid\n",
    "    return self\n",
    "\n",
    "  def getType(self):\n",
    "    return self.type\n",
    "\n",
    "  def createNeuron(self):\n",
    "    newNeuron = Neuron(self.type, self.bias)\n",
    "    self.neuron_array.append(newNeuron)\n",
    "    return self\n",
    "  \n",
    "  def getNeuron(self, index):\n",
    "    if(index < len(self.neuron_array)):\n",
    "      return self.neuron_array[index]\n",
    "    else:\n",
    "      return None\n",
    "  \n",
    "  def getNeuronArray(self):\n",
    "    return self.neuron_array\n",
    "  \n",
    "  def getActivationFunction(self):\n",
    "    return self.activationFunction\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O4NWKCjk2bJc"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "class MBGD:\n",
    "  # Masih kurang struktur jaringannya (jumlah layer, jumlah neuron setiap layer, fungsi aktivasi setiap layer)\n",
    "  def __init__(self, learning_rate=0.1, error_threshold=1.0048, max_iter=2000, batch_size=2, initial_weight=0.5):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.error_threshold = error_threshold\n",
    "    self.max_iter = max_iter\n",
    "    self.batch_size = batch_size\n",
    "    self.initial_weight = initial_weight\n",
    "    self.epoch = 0\n",
    "    self.error = 0\n",
    "    self.bias = 1\n",
    "    self.layerArray = None\n",
    "    \n",
    "  def setBias(self, bias):\n",
    "    self.bias = bias\n",
    "    return self\n",
    "  \n",
    "  def getBias(self):\n",
    "    return self.bias\n",
    "  \n",
    "  def getLayer(self, index):\n",
    "    if(len(self.layerArray) > index):\n",
    "      return self.layerArray[index]\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "  def createHiddenLayer(self, nNeuron, activation):\n",
    "    newLayer = Layer(self.bias)\n",
    "    newLayer.setType(activation)\n",
    "    for i in range (nNeuron):\n",
    "      newLayer.createNeuron()\n",
    "    return newLayer\n",
    "\n",
    "  def createOutputLayer(self, activation, totalDiffClass):\n",
    "    newLayer = Layer(self.bias)\n",
    "    newLayer.setType(activation)\n",
    "    if (activation == 4):\n",
    "      for i in range(totalDiffClass):\n",
    "        newLayer.createNeuron()\n",
    "    \n",
    "    else :\n",
    "      newLayer.createNeuron()\n",
    "    \n",
    "    return newLayer\n",
    "\n",
    "  def setLayer(self, layers):\n",
    "    newLayer = Layer(self.bias)\n",
    "    newLayer.setType(0)\n",
    "    self.layerArray = layers\n",
    "    self.layerArray.insert(0,newLayer)\n",
    "\n",
    "  def fit(self, dataset, label):\n",
    "    n_processed = 0\n",
    "    self.error = self.error_threshold + 1\n",
    "    while (self.epoch < self.max_iter and self.error > self.error_threshold):\n",
    "      # E = 0\n",
    "      update_weight = False\n",
    "      self.error = 0\n",
    "      for index_data, data in enumerate(dataset):\n",
    "        # FORWARD PROPAGATION\n",
    "        # result_output_layer = None\n",
    "        prevLayerInputArray = data\n",
    "\n",
    "        #if n process%batchsize = 0 updet w\n",
    "        if (self.batch_size != 0 and n_processed != 0 and n_processed % self.batch_size == 0):\n",
    "          for layer in self.layerArray:\n",
    "            for neuron in layer.getNeuronArray():\n",
    "              accumulative_delta = neuron.getAccumulativeDelta()\n",
    "              weight_arr = neuron.getWeightArray()\n",
    "              for index_weight in range(len(weight_arr)):\n",
    "                weight_arr[index_weight] += accumulative_delta[index_weight]\n",
    "              neuron.accumulative_delta = [0 for i in range(len(weight_arr))]\n",
    "                \n",
    "        if(n_processed != 0 and n_processed % len(dataset) == 0 and self.batch_size != len(dataset)):\n",
    "          for layer in self.layerArray:\n",
    "            for neuron in layer.getNeuronArray():\n",
    "              accumulative_delta = neuron.getAccumulativeDelta()\n",
    "              weight_arr = neuron.getWeightArray()\n",
    "              for index_weight in range(len(weight_arr)):\n",
    "                weight_arr[index_weight] += accumulative_delta[index_weight]\n",
    "              neuron.accumulative_delta = [0 for i in range(len(weight_arr))]\n",
    "\n",
    "        n_processed+=1\n",
    "\n",
    "        for layerNum in range(len(self.layerArray)):\n",
    "          layer = self.layerArray[layerNum]\n",
    "          nextLayerInputArray = []\n",
    "          nextLayerInputArray.append(self.bias)\n",
    "\n",
    "          # Input layer\n",
    "          if (layer.getType() == 0):\n",
    "            nextLayerInputArray.extend(data)\n",
    "            prevLayerInputArray = nextLayerInputArray.copy()\n",
    "            layer.output_array =  nextLayerInputArray.copy()\n",
    "\n",
    "          # hidden layer\n",
    "          else :\n",
    "            for neuron_idx in range(len(layer.getNeuronArray())):\n",
    "              neuron = layer.getNeuron(neuron_idx)\n",
    "\n",
    "              if (len(neuron.getWeightArray()) == 0):\n",
    "                neuron.setWeight([self.initial_weight for i in range(len(prevLayerInputArray))])\n",
    "                neuron.accumulative_delta = [0 for i in range(len(prevLayerInputArray))]\n",
    "\n",
    "              sigma = arrayMultiplication(prevLayerInputArray, neuron.getWeightArray(), len(prevLayerInputArray))\n",
    "              nextLayerInputArray.append(sigma)\n",
    "              # update weight (lRate * (yi - oi)*xi)\n",
    "              \n",
    "              # Last Neuron\n",
    "              if (neuron_idx == (len(layer.getNeuronArray())-1)):\n",
    "                # activate and append to nextLayer\n",
    "                nextLayerInputArray.pop(0)\n",
    "                \n",
    "                nextLayerInputArray = layer.activationFunction(nextLayerInputArray) # oi arr\n",
    "                nextLayerInputArray.insert(0, self.bias)\n",
    "                self.layerArray[layerNum].output_array = nextLayerInputArray\n",
    "                prevLayerInputArray = nextLayerInputArray\n",
    "\n",
    "        # BACKWARD\n",
    "        targetMinOutputArr = []\n",
    "        for index_output in range(len(nextLayerInputArray)):\n",
    "          # Target - Output\n",
    "          targetMinOutputArr.append(label[index_data] - nextLayerInputArray[index_output])\n",
    "        # Removing Bias\n",
    "        targetMinOutputArr.pop(0)\n",
    "        nextLayerInputArray.pop(0)\n",
    "\n",
    "        for diff_idx in range(len(targetMinOutputArr)):\n",
    "          diff = targetMinOutputArr[diff_idx]\n",
    "          self.error += diff*diff/2\n",
    "\n",
    "        for layerNum in range(len(self.layerArray) - 1, 0, -1):\n",
    "          layer = self.layerArray[layerNum]\n",
    "\n",
    "          # Pertama dia ada di output layer hitung error node output\n",
    "          if (layerNum == len(self.layerArray) - 1):\n",
    "\n",
    "            if (layer.type == 4): # SOFTMAX Here\n",
    "              for neuron_idx in range(len(layer.getNeuronArray())):\n",
    "                neuron = layer.getNeuron(neuron_idx)\n",
    "                neuron.errorNode = (-1) * derivate_Ed_To_NETj_Softmax(nextLayerInputArray[neuron_idx], label[index_data], neuron_idx)\n",
    "              #   print(neuron.errorNode)\n",
    "              # print(\"Softmax\")\n",
    "\n",
    "            else :\n",
    "              for neuron_idx in range(len(layer.getNeuronArray())):\n",
    "                neuron = layer.getNeuron(neuron_idx)\n",
    "                # Target, Output, Layer.type\n",
    "                neuron.errorNode = getErrorNodeOutput(label[index_data], nextLayerInputArray[neuron_idx], layer.type)\n",
    "\n",
    "          # Sekarang ada di layer hidden\n",
    "          else:\n",
    "            nextLayer = self.layerArray[layerNum + 1]\n",
    "            for neuron_idx in range(0, len(layer.getNeuronArray())):\n",
    "              neuron = layer.getNeuron(neuron_idx)\n",
    "              totalWkhDk = 0\n",
    "              for nextLayerNeuron_idx in range(len(nextLayer.getNeuronArray())):\n",
    "                nextLayerNeuron = nextLayer.getNeuron(nextLayerNeuron_idx)\n",
    "                thisNeuronWeight = nextLayerNeuron.getWeight(neuron_idx+1)\n",
    "                totalWkhDk += nextLayerNeuron.errorNode * thisNeuronWeight\n",
    "\n",
    "              neuron.errorNode = derivate_Ed_To_NETj(layer.type, totalWkhDk, layer.output_array[neuron_idx+1])\n",
    "\n",
    "        # Delta Weight Update\n",
    "        prevLayerInputArray = data\n",
    "        for layerNum in range(len(self.layerArray)):\n",
    "          layer = self.layerArray[layerNum]\n",
    "\n",
    "          # Input layer\n",
    "          if (layer.getType() == 0):\n",
    "            nextLayerInputArray.extend(data)\n",
    "            prevLayerInputArray = nextLayerInputArray.copy()\n",
    "\n",
    "          # Hidden Layer And output\n",
    "          else:\n",
    "            prevLayer = self.layerArray[layerNum - 1]\n",
    "            layer = self.layerArray[layerNum]\n",
    "            \n",
    "            for neuron_idx in range(len(layer.getNeuronArray())):\n",
    "              neuron = layer.getNeuron(neuron_idx)\n",
    "              accumulative_delta = neuron.getAccumulativeDelta()\n",
    "              layerNeuron = layer.getNeuron(neuron_idx)\n",
    "\n",
    "              for layerNeuron_idx in range(len(prevLayer.output_array)):\n",
    "                if (layerNeuron_idx < len(accumulative_delta)):\n",
    "                  accumulative_delta[layerNeuron_idx] += self.learning_rate * layerNeuron.errorNode * prevLayer.output_array[layerNeuron_idx]\n",
    "                else :\n",
    "                  accumulative_delta.append(self.learning_rate * layerNeuron.errorNode * prevLayer.output_array[layerNeuron_idx])\n",
    "              neuron.setAccumulativeDelta(accumulative_delta) \n",
    "      self.epoch += 1\n",
    "\n",
    "  # INI BEKAS YG FFNN\n",
    "  def predict(self, dataset):\n",
    "    output_sets = []\n",
    "    for data in dataset: \n",
    "      predicted_output = None\n",
    "\n",
    "      prevLayerInputArray = data\n",
    "      for layerNum in range(len(self.layerArray)):\n",
    "        layer = self.layerArray[layerNum]\n",
    "        nextLayerInputArray = []\n",
    "        nextLayerInputArray.append(self.bias)\n",
    "\n",
    "        if (layer.getType() == 0): #Input Layer\n",
    "          nextLayerInputArray.extend(data)\n",
    "          prevLayerInputArray = nextLayerInputArray.copy()\n",
    "          \n",
    "        else :\n",
    "          for neuron_idx in range(len(layer.getNeuronArray())) :\n",
    "            neuron = layer.getNeuron(neuron_idx)\n",
    "            sigma = arrayMultiplication(prevLayerInputArray, neuron.getWeightArray(), len(prevLayerInputArray))\n",
    "            \n",
    "            # output = neuron.activationFunction(sigma)\n",
    "            nextLayerInputArray.append(sigma)\n",
    "\n",
    "            # Last Neuron\n",
    "            if (neuron_idx == (len(layer.getNeuronArray())-1)):\n",
    "              # activate and append to nextLayer\n",
    "              nextLayerInputArray.pop(0)\n",
    "              nextLayerInputArray = layer.activationFunction(nextLayerInputArray)\n",
    "              nextLayerInputArray.insert(0, self.bias)\n",
    "              prevLayerInputArray = nextLayerInputArray\n",
    "\n",
    "        # Last Layer (Output Layer)\n",
    "        if (layerNum == (len(self.layerArray)-1)):\n",
    "          predicted_output = prevLayerInputArray\n",
    "\n",
    "    # Input the Predicted output to output set\n",
    "      output_sets.append(predicted_output[1:])\n",
    "    \n",
    "    return self.classify(output_sets, layer.type) \n",
    "\n",
    "  def printmodel(self):\n",
    "    i = 1\n",
    "    tab = \" \" \n",
    "    for layer in self.layerArray:\n",
    "      if (i == 1):\n",
    "        print(\"Layer Input\")\n",
    "      elif (i == len(self.layerArray)):\n",
    "        print(\"Layer output\")\n",
    "      else:\n",
    "        print(\"Layer \" + str(i))\n",
    "      i += 1\n",
    "      j = 0\n",
    "      for neuron in layer.neuron_array:\n",
    "        j += 1\n",
    "        print (2*tab + \"Neuron \" + str(j))\n",
    "        print(4*tab + \"Weight: \", end=\"\")\n",
    "        print(neuron.weight)\n",
    "\n",
    "\n",
    "  def classify(self, output, activation):\n",
    "    classified_output = []\n",
    "    for i in range(len(output)):\n",
    "      # Sigmoid && Linear && ReLU\n",
    "      if (activation == 1 or activation == 2 or activation == 3):\n",
    "        if (output[i][0] >= 0.5):\n",
    "          classified_output.append(1)\n",
    "        else:\n",
    "          classified_output.append(0)\n",
    "      # softmax\n",
    "      else:\n",
    "        maxIndex = 0\n",
    "        for j in range(len(output[i])):\n",
    "          if(output[i][maxIndex] < output[i][j]):\n",
    "            maxIndex = j\n",
    "        classified_output.append(maxIndex)\n",
    "\n",
    "    return classified_output\n",
    "\n",
    "  def confusion_matrix(self, y_test, pred):\n",
    "    # TN, TP, FN, FP\n",
    "    unique_val = unique(y_test)\n",
    "    confusion_matrix = []\n",
    "\n",
    "    for i in range(len(unique_val)):\n",
    "      confusion_matrix.append([0] * len(unique_val))\n",
    "\n",
    "    for y_idx in range(len(y_test)):\n",
    "      unique_index = unique_val.index(y_test[y_idx])\n",
    "\n",
    "      # Jika prediksinya benar\n",
    "      if (pred[y_idx] == y_test[y_idx]):\n",
    "        confusion_matrix[unique_index][unique_index] += 1\n",
    " \n",
    "      # Jika salah prediksi\n",
    "      else:\n",
    "        pred_index = unique_val.index(pred[y_idx])\n",
    "        confusion_matrix[unique_index][pred_index] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "  \n",
    "  def TP(self, c_matrix, index):\n",
    "    return c_matrix[index][index]\n",
    "\n",
    "  def TN(self, c_matrix, index):\n",
    "    count = 0\n",
    "    dim = len(c_matrix)\n",
    "    for i in range(dim):\n",
    "      for j in range(dim):\n",
    "        if (i != index or j != index):\n",
    "          count += 1\n",
    "    return count \n",
    "\n",
    "  def FP(self, c_matrix, index):\n",
    "    count = 0\n",
    "    dim = len(c_matrix)\n",
    "    for i in range(dim):\n",
    "      if(i != index):\n",
    "        count += c_matrix[index][i]\n",
    "    return count \n",
    "\n",
    "  def FN(self, c_matrix, index):\n",
    "    count = 0\n",
    "    dim = len(c_matrix)\n",
    "    for i in range(dim):\n",
    "      if(i != index):\n",
    "        count += c_matrix[i][index]\n",
    "    return count\n",
    "  \n",
    "  def precision(self, TP, FP):\n",
    "    if (TP == 0):\n",
    "      return 0\n",
    "    return (TP/(TP+FP))\n",
    "\n",
    "  def recall(self, TP, FN):\n",
    "    if (TP == 0):\n",
    "      return 0\n",
    "    return (TP/(TP+FN))\n",
    "\n",
    "  def F1(self, TP, FP, FN):\n",
    "    if (TP == 0):\n",
    "       return 0\n",
    "    return (2*TP/(2*TP + FP + FN))\n",
    "\n",
    "  def accuracy(self, c_matrix):\n",
    "    acc = 0\n",
    "    count = 0\n",
    "    dim = len(c_matrix)\n",
    "\n",
    "    for i in range(dim):\n",
    "      for j in range(dim):\n",
    "        count += c_matrix[i][j]\n",
    "\n",
    "    for i in range(dim):\n",
    "      acc += c_matrix[i][i]\n",
    "\n",
    "    if (acc == 0):\n",
    "      return 0\n",
    "    return acc/count\n",
    "  \n",
    "  def classification_report(self, c_matrix):\n",
    "    dim = len(c_matrix)\n",
    "    acc = 0\n",
    "    for i in range(len(c_matrix)):\n",
    "      print()\n",
    "      print(\"Class \", i)\n",
    "      print(\"Precission : \", self.precision(self.TP(c_matrix, i), self.FP(c_matrix, i)))\n",
    "      print(\"Recall     : \", self.recall(self.TP(c_matrix, i), self.FN(c_matrix, i)))\n",
    "      print(\"F1         : \", self.F1(self.TP(c_matrix, i), self.FP(c_matrix, i), self.FN(c_matrix, i)))\n",
    "    print()\n",
    "    print(\"Accuracy   :\", self.accuracy(c_matrix))\n",
    "    return \"finish\"\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oR2kP_g2bLw",
    "outputId": "405c64ae-8e42-4694-ed26-7d5bcd01418a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 0 2 2 1 2 0 0 0 1 0 0 2 2 2 2 2 1 2 1 0 2 2 0 0 2 0 2 2 1 1 2 2 0 1\n",
      " 1 2 1 2 1 0 0 0 2 0 1 2 2 0 0 1 0 2 1 2 2 1 2 2 1 0 1 0 1 1 0 1 0 0 2 2 2\n",
      " 0 0 1 0 2 0 2 2 0 2 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 2 0 0 2 1 2 1 2 2 1 2\n",
      " 0]\n",
      "Layer Input\n",
      "Layer 2\n",
      "  Neuron 1\n",
      "    Weight: [-3.460214549696176, -2.200072199680319, -2.205430959366223, 4.312970174353948, 6.188912922775574]\n",
      "  Neuron 2\n",
      "    Weight: [-3.460214549696176, -2.200072199680319, -2.205430959366223, 4.312970174353948, 6.188912922775574]\n",
      "Layer output\n",
      "  Neuron 1\n",
      "    Weight: [9.3354706682354, -15.306491105018138, -15.306491105018138]\n",
      "  Neuron 2\n",
      "    Weight: [6.92302707075409, 2.6728762844200498, 2.6728762844200498]\n",
      "  Neuron 3\n",
      "    Weight: [-14.758497738988915, 14.133614820598709, 14.133614820598709]\n",
      "[0, 1, 2, 0, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 0, 1, 2, 0, 0, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 1, 2, 2, 0, 2, 0]\n",
      "[[13, 0, 0], [0, 3, 13], [0, 0, 9]]\n",
      "\n",
      "Class  0\n",
      "Precission :  1.0\n",
      "Recall     :  1.0\n",
      "F1         :  1.0\n",
      "\n",
      "Class  1\n",
      "Precission :  0.1875\n",
      "Recall     :  1.0\n",
      "F1         :  0.3157894736842105\n",
      "\n",
      "Class  2\n",
      "Precission :  1.0\n",
      "Recall     :  0.4090909090909091\n",
      "F1         :  0.5806451612903226\n",
      "\n",
      "Accuracy   : 0.6578947368421053\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(\"./dataset.csv\")\n",
    "# label = df[\"label\"]\n",
    "# data = df.drop(columns=[\"label\"], inplace=False)\n",
    "# label = label.values\n",
    "# data = data.values\n",
    "\n",
    "# mbgd = MBGD.MBGD()\n",
    "# mbgd.setBias(1)\n",
    "\n",
    "# hidden1 = mbgd.createHiddenLayer(2, 3)\n",
    "# output = mbgd.createOutputLayer(2)\n",
    "\n",
    "# mbgd.setLayer([hidden1, output])\n",
    "\n",
    "# mbgd.fit(data, label)\n",
    "\n",
    "# mbgd.printmodel()\n",
    "# print(mbgd.predict(data))\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "dataset = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, random_state=1)\n",
    "\n",
    "print(y_train)\n",
    "\n",
    "mbgd = MBGD()\n",
    "mbgd.setBias(1)\n",
    "\n",
    "totalDiffTargetClass = len(unique(y_train))\n",
    "\n",
    "hidden1 = mbgd.createHiddenLayer(2, 2)\n",
    "output = mbgd.createOutputLayer(4, totalDiffTargetClass)\n",
    "\n",
    "mbgd.setLayer([hidden1, output])\n",
    "\n",
    "mbgd.fit(X_train, y_train)\n",
    "\n",
    "mbgd.printmodel()\n",
    "mbgd_pred = mbgd.predict(X_test) \n",
    "c_matrix = mbgd.confusion_matrix(y_test, mbgd_pred)\n",
    "\n",
    "print(mbgd_pred)\n",
    "print(c_matrix)\n",
    "print(mbgd.classification_report(c_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9yZRgRC2bOU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuSWRDY92bQk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pILOvjl62bSx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkgs9Z4L2bU6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TubesC_13518112.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
